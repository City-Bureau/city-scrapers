import json
from datetime import datetime

from city_scrapers_core.constants import NOT_CLASSIFIED
from city_scrapers_core.items import Meeting
from city_scrapers_core.spiders import CityScrapersSpider
from dateutil.relativedelta import relativedelta
from scrapy import Request


class ChiSsa60Spider(CityScrapersSpider):
    name = "chi_ssa_60"
    agency = "Chicago Special Service Area #60 Albany Park"
    timezone = "America/Chicago"
    months_backward = 6
    months_forward = 6

    start_urls = None  # Generated by `start_requests`
    custom_settings = {'ROBOTSTXT_OBEY': False}

    def start_requests(self):
        requests = []
        for i in range(-self.months_backward, self.months_forward + 1):
            dt = datetime.strftime(datetime.now().date() + relativedelta(months=i), "%Y-%m")
            requests.append(
                Request(
                    "https://northrivercommission.org/events/{}/".format(dt), callback=self.parse
                )
            )
        return requests

    def parse(self, response):
        """
        `parse` should always `yield` Meeting items.

        Change the `_parse_title`, `_parse_start`, etc methods to fit your scraping
        needs.
        """
        print(f"------------------------ START {response.url} ------------------------")
        sel_str = "//script[@type='application/ld+json']/text()"
        for item in response.xpath(sel_str):
            try:
                event_list = json.loads(item.get())
                for event_dict in event_list:
                    for k, v in event_dict.items():
                        print(f"{k}: {v}")
            except Exception as e:
                print(e)
        print(f"------------------------- END {response.url} -------------------------")
        print('\n')

        for item in response.css(".meetings"):
            meeting = Meeting(
                title=self._parse_title(item),
                description=self._parse_description(item),
                classification=self._parse_classification(item),
                start=self._parse_start(item),
                end=self._parse_end(item),
                all_day=self._parse_all_day(item),
                time_notes=self._parse_time_notes(item),
                location=self._parse_location(item),
                links=self._parse_links(item),
                source=self._parse_source(response),
            )

            meeting["status"] = self._get_status(meeting)
            meeting["id"] = self._get_id(meeting)

            yield meeting

    def _parse_title(self, item):
        """Parse or generate meeting title."""
        return ""

    def _parse_description(self, item):
        """Parse or generate meeting description."""
        return ""

    def _parse_classification(self, item):
        """Parse or generate classification from allowed options."""
        return NOT_CLASSIFIED

    def _parse_start(self, item):
        """Parse start datetime as a naive datetime object."""
        return None

    def _parse_end(self, item):
        """Parse end datetime as a naive datetime object. Added by pipeline if None"""
        return None

    def _parse_time_notes(self, item):
        """Parse any additional notes on the timing of the meeting"""
        return ""

    def _parse_all_day(self, item):
        """Parse or generate all-day status. Defaults to False."""
        return False

    def _parse_location(self, item):
        """Parse or generate location."""
        return {
            "address": "",
            "name": "",
        }

    def _parse_links(self, item):
        """Parse or generate links."""
        return [{"href": "", "title": ""}]

    def _parse_source(self, response):
        """Parse or generate source."""
        return response.url
